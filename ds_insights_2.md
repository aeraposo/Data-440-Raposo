Convolutional Neural Networks (CNN): The math behind the mystery<br/>
For the dawn of the first CNN, the inner workings of these algorithms are largely referred to as a “big black box”. The programmer knows what information goes in and what comes out but are often at a loss for how their computer arrived at its solution, classification, or prediction. Although there is more to these programmatic beasts than what can be unpacked in a brief “data science insights” assignment, I hope to illuminate a few particularly ‘mysterious’ areas in this vast and convoluted topic.<br/>
For clarity and consistency, we will primarily focus on a -----
The pieces: input layer receives pieces of training data, which is sent as vectors through the subsequent layers of the network. These training data contain measures of several relevant parameters that influence the final classification of the datapoint (the actual classification is paired with the training data). Depending on the contents of the vector passed through the layers, the neurons adjust their weights and biases. Weights and biases are values that gauge the likelihood of a piece of data being of a certain classification or similar prediction. This allows the model to learn to recognize similar features within novel data and make appropriate predictions. Sounds simple enough, right? Although these concepts make sense from a high level, you may wonder, how is a weight or bias actually computed and what is their mathematical significance? Great question! A neuron’s weight is a value, which represents a weighted average of the data contained in the vector sent to the neron. A weighted average is a calculation of the mean of data after placing coefficients of relative importance on each component (ie- if the model sees that feature A is highly correlated with classifying the data its is given, it will place a higher weight/ multiply this feature by a higher value before computing the average). Next, a bias added to the calculated weight. Biases are constants that help add specificity to the network and correct for flaws in classification or predictions, which are discovered by the model as it learns. For example, if a modelI know is overly sensitive and is over classifying or over-predicted because of characteristics of a specific area of the data, it may appropriately adjust its weights in an effort to correct for this error. Next, this value (the weighted average plus the bias) is input into an activation function, which decides if a neuron should be ‘activated’ or not. Looking at the neurons’ activations is what helps the model determine how it should classify a datapoint. Activation functions are nonlinear, which increases the network’s ability to make dynamic predictions. Some popular activation functions include ReLU and Sigmoid (used in binary classifications). After data data is successfully passed from the input layer to the first dense layer and the above calculations are performed, the value generated by the activation function for every neuron is then passed into every other neuron of the following layer, forming a new set of vectors for the layer of neurons to process. This process repeats until the network’s output layer is reached. In this layer, based on the activation values passed in via vectors to the final layer, these output neurons are assigned (in the case of a classification network) a value between 0 and 1, which describes the the probability that a datapoint of that would traverse the neural network as such would be classified as the assigned label of each output neuron. Which every value is highest in the model’s final prediction of a datapoint’s classification. In short, each datapoint, when sent through the network, will result in a distinct pattern of activation values. Similarities in how new and old data traverse the network, which is recorded by the values of vectors of outputs of activation functions and the values assigned to neurons, is how the model is able to assign these probabilities in the terminal/output nodes.
