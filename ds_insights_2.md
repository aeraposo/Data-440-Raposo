**Convolutional Neural Networks (CNN): The math behind the mystery**<br/>

From the dawn of the first CNN, the inner workings of these algorithms have largely been referred to as a “big black box”. In short, programmers knows what information goes in and what they want to be output, but most are at a loss for how computers arrive at a solution, classification, or prediction. Although there is more to these programmatic beasts than what can be unpacked in a brief “data science insights” assignment, I hope to illuminate a few particularly ‘mysterious’ areas in this vast and convoluted topic.<br/>
![cnn](https://aeraposo.github.io/Data-440-Raposo/cnn.png)[Image source](https://www.houseofbots.com/news-detail/12080-1-learn-from-the-industry-experts-and-professionals-to-grow-a-career-in-data-science)<br/>

Beginning at the 'front' of the network, the input layer receives pieces of training data, which is sent (piece by piece) as vectors through the subsequent layers of the network. These training data contain measures of several relevant parameters that influence the final classification of each datapoint (the actual classification is paired with the training data so the model can 'check its answers'). Depending on the contents of the vector passed through the layers, the neurons adjust their weights and biases accordingly. Weights and biases are values that help the model gauge the likelihood of a piece of data being of a certain classification or prediction. This allows the model to learn to recognize similar features within novel data and make appropriate predictions. Sounds simple enough, right? Although these concepts make sense from a high level, you may wonder, how is a weight or bias actually computed and what are their mathematical significances? Great question! A neuron’s weight is a value, which represents a weighted average of the data contained in the vector sent to the neron. A weighted average is a calculation of the mean of data after placing coefficients of relative importance on each component (ie- if the model sees that feature A is highly correlated with classifying the data its is given, it will place a higher weight/ multiply this feature by a higher value before computing the average). Next, a bias added to the calculated weight. Biases are constants that help add specificity to the network and correct for flaws in classification or predictions, which are discovered by the model as it learns. For example, if a model is overly sensitive and over-classifies/predicts because of characteristics of a specific area of the data, it may appropriately adjust its weights in an effort to correct for this error. Next, this value (the weighted average plus the bias) is input into an activation function, which decides if a neuron should be ‘activated’ or not. Looking at the neurons’ activations is what helps the model determine how it should classify a datapoint. Activation functions are nonlinear, which increases the network’s ability to make dynamic predictions. Some popular activation functions include ReLU and Sigmoid (particularly, Sigmoid is used in binary classifications).<br/>
![neuron](https://aeraposo.github.io/Data-440-Raposo/neuron.png)[Image source](https://towardsdatascience.com/https-medium-com-piotr-skalski92-deep-dive-into-deep-networks-math-17660bc376ba)<br/>

After data is successfully passed from the input layer to the first dense layer and the above calculations are performed, the value generated by the activation function for every neuron is then passed into every other neuron of the following layer, forming a new set of vectors for the next layer of neurons to process. This process repeats until the network’s output layer is reached. In this layer, values are assigned to the output neurons based on the activation values passed in via vectors to the final layer. Theses terminal neurons' values, in the case of a classification network, fall between 0 and 1 and describe the the probability that a datapoint is a given classification (each terminal node represents the probability of being a different classification). So any datapoint that traverses the neural network as such would be classified based on the predicted probabilties of the nerons paired with each possible label. Which ever probabilty is highest in the output layer determines the model’s final prediction of a datapoint’s classification. In short, each datapoint, when sent through the network, will result in a distinct pattern of activation values. Similarities in how new and old data traverse the network, which is recorded by the values of vectors of outputs of activation functions and the values assigned to neurons, is how the model is able to assign these probabilities in the terminal/output nodes, which ultimately determines the classficiation of the datapoint. This entire process repeats with every convolution of the network and allows the model to learn gradually, of course with the help of a LOT of mathemagic.<br/>

[Source 1](https://towardsdatascience.com/https-medium-com-piotr-skalski92-deep-dive-into-deep-networks-math-17660bc376ba), [source 2](https://www.geeksforgeeks.org/activation-functions-neural-networks/).
